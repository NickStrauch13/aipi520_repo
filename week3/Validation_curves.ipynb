{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Curves\n",
    "\n",
    "In this notebook we will look at one technique for comparing models of varying complexity using what is called a validation curve.  Validation curves can help us determine where the optimal tradeoff between bias and variance is for a given dataset and algorithm.  \n",
    "\n",
    "We will again use the sonar dataset we have previously used.  Details on the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we know we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(url, header=None)\n",
    "\n",
    "    # Separate into X and y \n",
    "    # Create feature matrix using the first 60 columns as the features\n",
    "    X = data.iloc[:,:60].to_numpy()\n",
    "    # Create target vector from the last column\n",
    "    y = data.iloc[:,60].to_numpy()\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X,y = load_data('https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X,y,pct):\n",
    "    # Split the data into training and test sets\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=pct)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split(X,y,pct=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model - in this case we will use a Logistic Regression classification model\n",
    "model = LogisticRegression(penalty='l2',C=0.5,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, our Logistic Regression model has a hyperparameter C (the inverse of the regularization strength).  Applying more regularization (lower C) yields a simpler model, and less regularization (higher C) yields a more complex model.  We can use a validation curve to visualize the bias-variance tradeoff as we vary the value of C and select the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the range of values we want to evaluate for the hyperparameter min_samples_leaf\n",
    "C_range=np.arange(0,1.,0.05)\n",
    "\n",
    "# Set up the validation curve\n",
    "train_scores,val_scores = validation_curve(estimator=model,\n",
    "                                           X=X_train,\n",
    "                                           y=y_train,\n",
    "                                           param_name='C',\n",
    "                                           param_range=C_range,\n",
    "                                           cv=10,\n",
    "                                          scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean across the folds for the training accuracy and validation accuracy\n",
    "train_means = np.mean(train_scores,axis=1)\n",
    "val_means = np.mean(val_scores,axis=1)\n",
    "\n",
    "# Plot the mean training accuracy and validation accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(C_range,train_means,color='blue',marker='o',label='Training accuracy')\n",
    "plt.plot(C_range,val_means,color='green',marker='s',label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.xticks(ticks=np.arange(0,1,0.1))\n",
    "plt.xlabel('C (Inverse of regularization strength)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there appears to be an optimal value of C around 0.2.  Above this, the training accuracy and validation accuracy diverge - training accuracy continues to improve as the model becomes more complex, but the validation accuracy remains flat or decreases slightly.  This is a good indication that the more complex models are overfitting the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aipi540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
