{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to look at strategies to divide your dataset in order to perform model selection and testing using subsets of data in ways that do not create bias in your measurement of model performance.\n",
    "\n",
    "We are going to use a dataset which comes from a study done to try to use sonar signals to differentiate between a mine (simulated using a metal cylinder) and a rock.  Details on the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we know we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
    "data = pd.read_csv(url, header=None)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training and test sets\n",
    "In this part, you should complete the following:  \n",
    "- Split your data into a feature matrix X and a target vector y  \n",
    "- Split the data into a training set and a test set, using 85% of the data for training and 15% for testing (hint: use scikit-learn's train_test_split() method, already imported for you.  Name the resulting arrays `X_train, y_train, X_test, y_test`\n",
    "- Train (fit) your model on the X and y training sets  \n",
    "- Use your trained model to get predictions on the `X_test` test set, and name the predictions `preds`  \n",
    "- Finally, run the next code cell to calculate the display the accuracy of your classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data,model):\n",
    "    '''\n",
    "    Splits the data, trains a model on the training data and then generates and returns predictions on the test set\n",
    "\n",
    "    Inputs:\n",
    "        data(DataFrame): dataframe containing the data features and labels\n",
    "        model(sklearn.base.BaseEstimator): instantiated scikit-learn model object\n",
    "\n",
    "    Returns:\n",
    "        preds(np.ndarray): numpy array containing the model predictions for the test set\n",
    "        y_test(np.ndarray): numpy array containing the labels for the test set\n",
    "    '''\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "    \n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLPClassifier algorithm and set the hyperparameter values\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,50,10),activation='tanh',\n",
    "                      solver='sgd',learning_rate_init=0.001,max_iter=2000, random_state=0)\n",
    "                      \n",
    "# Evaluate the performance of our model using the test predictions\n",
    "preds,y_test = run_model(data,model)\n",
    "assert len(preds) == len(y_test)\n",
    "acc_test = np.sum(preds==y_test)/len(y_test)\n",
    "print('Accuracy of our classifier on the test set is {:.3f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model selection using validation sets\n",
    "But what if we want to compare different models (for example, evaluate different algorithms or fine-tune our hyperparameters)?  Can we use the same strategy of training each model on the training data and then comparing their performance on the test set to select the best model?\n",
    "\n",
    "When we are seeking to optimize models by tuning hyperparameters or comparing different algorithms, it is a best practice to do so by comparing the performance of your model options using a \"validation\" set, and then reserve use of the test set to evaluate the performance of the final model you have selected.  To utilize this approach we must split our data three ways to create a training set, validation set, and test set.\n",
    "\n",
    "To illustrate this, let's compare two different models.  Complete the function below which performs the following:\n",
    "- Split your data into X and y arrays and then into a training set and a test set, using 15% of data for the test set.  Store the training data as `X_train_full, y_train_full` and the test set data as `X_test, y_test`\n",
    "- Now, split your training set again into a training set and a validation set, using 15% of the training set for the new validation set (and the remaining 85% is still available for training). Store the final training data as `X_train, y_train` and the validation set data as `X_val, y_val`\n",
    "- Train (fit) model1 and model2 using the training data only  \n",
    "- Now, use your trained model1 and model2 to generate predictions on the validation set.  Store model1's predictions as `val_preds_model1` and model2's predictions as `val_preds_model2`  \n",
    "- Finally, run the code cell below to calculate the accuracy of each on the validation set.  Based on this, which model would you select as your final model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(data, model1, model2):\n",
    "    '''\n",
    "    Splits data into training, validation and test sets, trains two models and then generates predictions on the validation set using both models\n",
    "\n",
    "    Inputs:\n",
    "        data(DataFrame): dataframe containing the data features and labels\n",
    "        model1(sklearn.base.BaseEstimator): instantiated scikit-learn model object\n",
    "        model2(sklearn.base.BaseEstimator): instantiated scikit-learn model object\n",
    "\n",
    "    Returns:\n",
    "        val_preds_model1(np.ndarray): numpy array containing the model predictions for the test set\n",
    "        val_preds_model2(np.ndarray): numpy array containing the model predictions for the test set\n",
    "        y_val(np.ndarray): numpy array containing the labels for the validation set\n",
    "        y_test(np.ndarray): numpy array containing the labels for the test set\n",
    "    '''\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    \n",
    "\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of each model we want to evaluate\n",
    "\n",
    "model1 = MLPClassifier(hidden_layer_sizes=(100,50,10),activation='tanh',\n",
    "                      solver='sgd',learning_rate_init=0.001,max_iter=2000, random_state=0)\n",
    "\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=2000, random_state=0)\n",
    "\n",
    "# Calculate the validation accuracy of each model\n",
    "val_preds_model1, val_preds_model2, y_val, y_test = compare_models(data, model1, model2)\n",
    "acc_val_model1 = np.sum(val_preds_model1==y_val)/len(y_val)\n",
    "acc_val_model2 = np.sum(val_preds_model2==y_val)/len(y_val)\n",
    "\n",
    "print('Accuracy of model1 on the validation set is {:.3f}'.format(acc_val_model1))\n",
    "print('Accuracy of model2 on the validation set is {:.3f}'.format(acc_val_model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've chosen our final model, we can use the test set to evaluate it's performance.  Before we do that, let's retrain our model using the training plus validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our selected model on the training plus validation sets\n",
    "preds,y_test = run_model(data,model2)\n",
    "\n",
    "# Evaluate its performance on the test set\n",
    "acc_test = np.sum(preds==y_test)/len(y_test)\n",
    "print('Accuracy of our model on the test set is {:.3f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model selection using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to comparing and optimizing models is to use cross-validation rather than a single validation set to compare model performace.  We will then select the better model based on the cross-validation performance and use the test set to determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crossvalidation(X_train,y_train,models):\n",
    "    '''\n",
    "    Performs k-folds cross validation on an arbitrary number of models provided as inputs and returns the mean cross-validation accuracy of each\n",
    "\n",
    "    Inputs:\n",
    "        X_train(np.ndarray): numpy array containing the training set features\n",
    "        y_train(np.ndarray): numpy array containing the training set labels\n",
    "        models(list): list of instantiated scikit-learn model objects\n",
    "\n",
    "    Returns:\n",
    "        crossval_accs(list): list containing the mean cross-validation accuracy of each model across the validation folds\n",
    "    '''\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    \n",
    "        \n",
    "    ### END SOLUTION ###\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set aside a test set and use the remainder for training and cross-validation\n",
    "X = data.iloc[:,:60].to_numpy()\n",
    "y = data.iloc[:,60].to_numpy()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.15)\n",
    "\n",
    "# Set up the two models we want to compare: a neural network model and a KNN model\n",
    "model_a = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=1000,random_state=0)\n",
    "\n",
    "model_b = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "models = [model_a, model_b]\n",
    "\n",
    "accs = run_crossvalidation(X_train,y_train,models)\n",
    "for model,acc in zip(models,accs):\n",
    "    print('For model: {}'.format(model))\n",
    "    print('Mean cross-validation accuracy across all folds is {:.3f} \\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set aside a test set and use the remainder for training and cross-validation\n",
    "X = data.iloc[:,:60].to_numpy()\n",
    "y = data.iloc[:,60].to_numpy()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.15)\n",
    "\n",
    "# Set up the two models we want to compare: a neural network model and a KNN model\n",
    "model_a = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=1000,random_state=0)\n",
    "\n",
    "model_b = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "models = [model_a, model_b]\n",
    "\n",
    "# Cross-validation using cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "for model in models:\n",
    "    scores = cross_val_score(model,X_train,y_train,scoring=\"accuracy\",cv=3)\n",
    "    mean_score = np.mean(scores)\n",
    "    print(model)\n",
    "    print('Mean cross-validation accuracy across all folds is {:.3f} \\n'.format(mean_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the cross-validation accuracy of model_a is higher than model_b, so we will use model_a.  Let's now evaluate the performance of model_a on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our selected model on the training plus validation sets\n",
    "preds,y_test = run_model(data,model_a)\n",
    "\n",
    "# Evaluate its performance on the test set\n",
    "acc_test = np.sum(preds==y_test)/len(y_test)\n",
    "print('Accuracy of our model on the test set is {:.3f}'.format(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aipi540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
